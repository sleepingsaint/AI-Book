{
    "hasNextPage": true,
    "data": [
        {
            "id": 9022,
            "title": "Recommendations for Deep Learning Neural Network Practitioners",
            "url": "https://machinelearningmastery.com/recommendations-for-deep-learning-neural-network-practitioners/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-25T00:00:00",
            "description": "Deep learning neural networks are relatively straightforward to define and train given the wide adoption of open source libraries. Nevertheless, neural networks remain challenging to configure and train. In his 2012 paper titled \u201cPractical Recommendations for Gradient-Based Training of Deep Architectures\u201d published as a preprint and a chapter of the popular 2012 book \u201cNeural Networks: [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/02/Practical-Recommendations-for-Deep-Learning-Neural-Network-Practitioners.jpg"
        },
        {
            "id": 9023,
            "title": "8 Tricks for Configuring Backpropagation to Train Better Neural Networks",
            "url": "https://machinelearningmastery.com/best-advice-for-configuring-backpropagation-for-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-22T00:00:00",
            "description": "Neural network models are trained using stochastic gradient descent and model weights are updated using the backpropagation algorithm. The optimization solved by training a neural network model is very challenging and although these algorithms are widely used because they perform so well in practice, there are no guarantees that they will converge to a good [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/02/8-Tricks-for-Configuring-Backpropagation-to-Train-Better-Neural-Networks-Faster.jpg"
        },
        {
            "id": 9024,
            "title": "Neural Networks: Tricks of the Trade Review",
            "url": "https://machinelearningmastery.com/neural-networks-tricks-of-the-trade-review/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-20T00:00:00",
            "description": "Deep learning neural networks are challenging to configure and train. There are decades of tips and tricks spread across hundreds of research papers, source code, and in the heads of academics and practitioners. The book \u201cNeural Networks: Tricks of the Trade\u201d originally published in 1998 and updated in 2012 at the cusp of the deep [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Neural-Networks-Tricks-of-the-Trade.jpg"
        },
        {
            "id": 9025,
            "title": "How to Get Better Deep Learning Results (7-Day Mini-Course)",
            "url": "https://machinelearningmastery.com/better-deep-learning-neural-networks-crash-course/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-18T00:00:00",
            "description": "Better Deep Learning Neural Networks Crash Course. Get Better Performance From Your Deep Learning Models in 7 Days. Configuring neural network models is often referred to as a \u201cdark art.\u201d This is because there are no hard and fast rules for configuring a network for a given problem. We cannot analytically calculate the optimal model [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/02/How-to-Get-Better-Deep-Learning-Performance-7-Day-Mini-Course.jpg"
        },
        {
            "id": 9026,
            "title": "A Gentle Introduction to the Challenge of Training Deep Learning Neural Network Models",
            "url": "https://machinelearningmastery.com/a-gentle-introduction-to-the-challenge-of-training-deep-learning-neural-network-models/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-15T00:00:00",
            "description": "Deep learning neural networks learn a mapping function from inputs to outputs. This is achieved by updating the weights of the network in response to the errors the model makes on the training dataset. Updates are made to continually reduce this error until either a good enough model is found or the learning process gets [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/02/A-Gentle-Introduction-to-the-Challenge-of-Training-Deep-Learning-Neural-Network-Models.jpg"
        },
        {
            "id": 9027,
            "title": "How to Control Neural Network Model Capacity With Nodes and Layers",
            "url": "https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-13T00:00:00",
            "description": "The capacity of a deep learning neural network model controls the scope of the types of mapping functions that it is able to learn. A model with too little capacity cannot learn the training dataset meaning it will underfit, whereas a model with too much capacity may memorize the training dataset, meaning it will overfit [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Line-Plot-of-Cross-Entropy-Loss-Over-Training-Epochs-for-an-MLP-on-the-Training-Dataset-for-the-Blobs-Multi-Class-Classification-Problem-When-Varying-Model-Nodes.png"
        },
        {
            "id": 9028,
            "title": "Framework for Better Deep Learning",
            "url": "https://machinelearningmastery.com/framework-for-better-deep-learning/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-11T00:00:00",
            "description": "Modern deep learning libraries such as Keras allow you to define and start fitting a wide range of neural network models in minutes with just a few lines of code. Nevertheless, it is still challenging to configure a neural network to get good performance on a new predictive modeling problem. The challenge of getting good [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/02/Framework-for-Better-Deep-Learning.jpg"
        },
        {
            "id": 9029,
            "title": "Your First Machine Learning Project in Python Step-By-Step",
            "url": "https://machinelearningmastery.com/machine-learning-in-python-step-by-step/",
            "authors": "Jason Brownlee",
            "tags": "Python Machine Learning",
            "publishedOn": "2019-02-10T00:00:00",
            "description": "Do you want to do machine learning using Python, but you\u2019re having trouble getting started? In this post, you will complete your first machine learning project using Python. In this step-by-step tutorial you will: Download and install Python SciPy and get the most useful package for machine learning in Python. Load a dataset and understand [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2016/06/Your-First-Machine-Learning-Project-in-Python-Step-By-Step.jpg"
        },
        {
            "id": 9030,
            "title": "How to Improve Performance With Transfer Learning for Deep Learning Neural Networks",
            "url": "https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-08T00:00:00",
            "description": "An interesting benefit of deep learning neural networks is that they can be reused on related problems. Transfer learning refers to a technique for predictive modeling on a different but somehow similar problem that can then be reused partly or wholly to accelerate the training and improve the performance of a model on the problem [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Loss-and-Accuracy-Learning-Curves-on-the-Train-and-Test-Sets-for-an-MLP-on-Problem-1.png"
        },
        {
            "id": 9031,
            "title": "How to Avoid Exploding Gradients With Gradient Clipping",
            "url": "https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-06T00:00:00",
            "description": "Training a neural network can become unstable given the choice of error function, learning rate, or even the scale of the target variable. Large updates to weights during training can cause a numerical overflow or underflow often referred to as \u201cexploding gradients.\u201d The problem of exploding gradients is more common with recurrent neural networks, such [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/02/How-to-Avoid-Exploding-Gradients-in-Neural-Networks-With-Gradient-Clipping.jpg"
        },
        {
            "id": 9032,
            "title": "How to use Data Scaling Improve Deep Learning Model Stability and Performance",
            "url": "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-04T00:00:00",
            "description": "Deep learning neural networks learn how to map inputs to outputs from examples in a training dataset. The weights of the model are initialized to small random values and updated via an optimization algorithm in response to estimates of error on the training dataset. Given the use of small weights in the model and the [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Box-and-Whisker-Plots-of-Mean-Squared-Error-With-Unscaled-Normalized-and-Standardized-Input-Variables-for-the-Regression-Problem.png"
        },
        {
            "id": 9033,
            "title": "How to Use Greedy Layer-Wise Pretraining in Deep Learning Neural Networks",
            "url": "https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-02-01T00:00:00",
            "description": "Training deep neural networks was traditionally challenging as the vanishing gradient meant that weights in layers close to the input layer were not updated in response to errors calculated on the training dataset. An innovation and important milestone in the field of deep learning was greedy layer-wise pretraining that allowed very deep neural networks to [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Line-Plot-for-Supervised-Greedy-Layer-Wise-Pretraining-Showing-Model-Layers-vs-Train-and-Test-Set-Classification-Accuracy-on-the-Blobs-Classification-Problem.png"
        },
        {
            "id": 9034,
            "title": "How to Choose Loss Functions When Training Deep Learning Neural Networks",
            "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-30T00:00:00",
            "description": "Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm. As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly. This requires the choice of an error function, conventionally called a loss function, that can be used to estimate the loss of the [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Line-Plots-of-KL-Divergence-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Blobs-Multi-Class-Classification-Problem.png"
        },
        {
            "id": 9035,
            "title": "Loss and Loss Functions for Training Deep Learning Neural Networks",
            "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-28T00:00:00",
            "description": "Neural networks are trained using stochastic gradient descent and require that you choose a loss function when designing and configuring your model. There are many loss functions to choose from and it can be challenging to know what to choose, or even what a loss function is and the role it plays when training a [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/01/Loss-and-Loss-Functions-for-Training-Deep-Learning-Neural-Networks.jpg"
        },
        {
            "id": 9036,
            "title": "Understand the Impact of Learning Rate on Neural Network Performance",
            "url": "https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-25T00:00:00",
            "description": "Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Line-Plots-of-Train-and-Test-Accuracy-for-a-Suite-of-Learning-Rates-on-the-Blobs-Classification-Problem.png"
        },
        {
            "id": 9037,
            "title": "How to Configure the Learning Rate When Training Deep Learning Neural Networks",
            "url": "https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-23T00:00:00",
            "description": "The weights of a neural network cannot be calculated using an analytical method. Instead, the weights must be discovered via an empirical optimization procedure called stochastic gradient descent. The optimization problem addressed by stochastic gradient descent for neural networks is challenging and the space of solutions (sets of weights) may be comprised of many good [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/01/How-to-Configure-the-Learning-Rate-Hyperparameter-When-Training-Deep-Learning-Neural-Networks.jpg"
        },
        {
            "id": 9038,
            "title": "How to Control the Stability of Training Neural Networks With the Batch Size",
            "url": "https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-21T00:00:00",
            "description": "Neural networks are trained using gradient descent where the estimate of the error used to update the weights is calculated based on a subset of the training dataset. The number of examples from the training dataset used in the estimate of the error gradient is called the batch size and is an important hyperparameter that [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Line-Plots-of-Classification-Accuracy-on-Train-and-Test-Datasets-With-Different-Batch-Sizes.png"
        },
        {
            "id": 9039,
            "title": "How to Accelerate Learning of Deep Neural Networks With Batch Normalization",
            "url": "https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-18T00:00:00",
            "description": "Batch normalization is a technique designed to automatically standardize the inputs to a layer in a deep learning neural network. Once implemented, batch normalization has the effect of dramatically accelerating the training process of a neural network, and in some cases improves the performance of the model via a modest regularization effect. In this tutorial, [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/11/Line-Plot-Classification-Accuracy-of-MLP-with-Batch-Normalization-After-Activation-Function-on-Train-and-Test-Datasets-over-Training-Epochs.png"
        },
        {
            "id": 9040,
            "title": "Practical Deep Learning for Coders (Review)",
            "url": "https://machinelearningmastery.com/practical-deep-learning-for-coders-review/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning",
            "publishedOn": "2019-01-17T00:00:00",
            "description": "Practical deep learning is a challenging subject in which to get started. It is often taught in a bottom-up manner, requiring that you first get familiar with linear algebra, calculus, and mathematical optimization before eventually learning the neural network techniques. This can take years, and most of the background theory will not help you to [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2019/01/Overview-of-course-Structure.png"
        },
        {
            "id": 9041,
            "title": "A Gentle Introduction to Batch Normalization for Deep Neural Networks",
            "url": "https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-16T00:00:00",
            "description": "Training deep neural networks with tens of layers is challenging as they can be sensitive to the initial random weights and configuration of the learning algorithm. One possible reason for this difficulty is the distribution of the inputs to layers deep in the network may change after each mini-batch when the weights are updated. This [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2020/02/How-to-Calibrate-Probabilities-for-Imbalanced-Classification.jpg"
        },
        {
            "id": 9042,
            "title": "3 Must-Own Books for Deep Learning Practitioners",
            "url": "https://machinelearningmastery.com/books-for-deep-learning-practitioners/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-14T00:00:00",
            "description": "Developing neural networks is often referred to as a dark art. The reason for this is that being skilled at developing neural network models comes from experience. There are no reliable methods to analytically calculate how to design a \u201cgood\u201d or \u201cbest\u201d model for your specific dataset. You must draw on experience and experiment in [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Neural-Networks-for-Pattern-Recognition.jpg"
        },
        {
            "id": 9043,
            "title": "How to Fix the Vanishing Gradients Problem Using the ReLU",
            "url": "https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-11T00:00:00",
            "description": "The vanishing gradients problem is one example of unstable behavior that you may encounter when training a deep neural network. It describes the situation where a deep multilayer feed-forward network or a recurrent neural network is unable to propagate useful gradient information from the output end of the model back to the layers near the [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Train-and-Test-Set-Accuracy-of-Over-Training-Epochs-for-Deep-MLP-with-ReLU-with-15-Hidden-Layers.png"
        },
        {
            "id": 9044,
            "title": "A Gentle Introduction to the Rectified Linear Unit (ReLU)",
            "url": "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-09T00:00:00",
            "description": "In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input. The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png"
        },
        {
            "id": 9045,
            "title": "Ensemble Neural Network Model Weights in Keras (Polyak Averaging)",
            "url": "https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-07T00:00:00",
            "description": "The training process of neural networks is a challenging optimization process that can often fail to converge. This can mean that the model at the end of training may not be a stable or best-performing set of weights to use as a final model. One approach to address this problem is to use an average [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Single-Model-Test-Performance-blue-dots-and-Model-Weight-Ensemble-Test-Performance-orange-line-with-a-Exponential-Decay.png"
        },
        {
            "id": 9046,
            "title": "Snapshot Ensemble Deep Learning Neural Network in Python",
            "url": "https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-04T00:00:00",
            "description": "Model ensembles can achieve lower generalization error than single models but are challenging to develop with deep learning neural networks given the computational cost of training each single model. An alternative is to train multiple model snapshots during a single training run and combine their predictions to make an ensemble prediction. A limitation of this [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Cosine-Annealing-Learning-Rate-Schedule.png"
        },
        {
            "id": 9047,
            "title": "Impact of Dataset Size on Deep Learning Model Skill And Performance Estimates",
            "url": "https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2019-01-02T00:00:00",
            "description": "Supervised learning is challenging, although the depths of this challenge are often learned then forgotten or willfully ignored. This must be the case, because dwelling too long on this challenge may result in a pessimistic outlook. In spite of the challenge, we continue to wield supervised learning algorithms and they perform well in practice. Fundamental [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Four-Scatter-Plots-of-the-Circles-Dataset-Varied-by-the-Amount-of-Statistical-Noise.png"
        },
        {
            "id": 9048,
            "title": "Stacking Ensemble for Deep Learning Neural Networks in Python",
            "url": "https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2018-12-31T00:00:00",
            "description": "Model averaging is an ensemble technique where multiple sub-models contribute equally to a combined prediction. Model averaging can be improved by weighting the contributions of each sub-model to the combined prediction by the expected performance of the submodel. This can be extended further by training an entirely new model to learn how to best combine [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Visualization-of-Stacked-Generalization-Ensemble-of-Neural-Network-Models.png"
        },
        {
            "id": 9049,
            "title": "How to Develop a Weighted Average Ensemble for Deep Learning Neural Networks",
            "url": "https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2018-12-28T00:00:00",
            "description": "A modeling averaging ensemble combines the prediction from each model equally and often results in better performance on average than a given single model. Sometimes there are very good models that we wish to contribute more to an ensemble prediction, and perhaps less skillful models that may be useful but should contribute less to an [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-Learning-Curves-of-Model-Accuracy-on-Train-and-Test-Dataset-over-Each-Training-Epoch-3.png"
        },
        {
            "id": 9050,
            "title": "How to Develop a Horizontal Voting Deep Learning Ensemble to Reduce Variance",
            "url": "https://machinelearningmastery.com/horizontal-voting-ensemble/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2018-12-26T00:00:00",
            "description": "Predictive modeling problems where the training dataset is small relative to the number of unlabeled examples are challenging. Neural networks can perform well on these types of problems, although they can suffer from high variance in model performance as measured on a training or hold-out validation datasets. This makes choosing which model to use as [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-Showing-Single-Model-Accuracy-blue-dots-vs-Accuracy-of-Ensembles-of-Varying-Size-with-a-Horizontal-Voting-Ensemble.png"
        },
        {
            "id": 9051,
            "title": "How to Create a Bagging Ensemble of Deep Learning Models in Keras",
            "url": "https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/",
            "authors": "Jason Brownlee",
            "tags": "Deep Learning Performance",
            "publishedOn": "2018-12-24T00:00:00",
            "description": "Ensemble learning are methods that combine the predictions from multiple models. It is important in ensemble learning that the models that comprise the ensemble are good, making different prediction errors. Predictions that are good in different ways can result in a prediction that is both more stable and often better than the predictions of any [\u2026]",
            "thumbnail": "https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-Showing-Single-Model-Accuracy-blue-dots-vs-Accuracy-of-Ensembles-of-Varying-Size-for-Bagging.png"
        }
    ]
}